#!/usr/bin/env python3
"""
è¨˜æ†¶é«”è¨ºæ–·å·¥å…·
ç”¨æ–¼åˆ†æ Breeze-ASR-25 å¾®èª¿çš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
"""

import gc
import os
from pathlib import Path

import numpy as np
import psutil
import torch


def print_memory_status():
    """é¡¯ç¤ºç•¶å‰è¨˜æ†¶é«”ç‹€æ…‹"""
    print("=" * 60)
    print("ğŸ” è¨˜æ†¶é«”ç‹€æ…‹è¨ºæ–·")
    print("=" * 60)

    # ç³»çµ±è¨˜æ†¶é«”
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ç³»çµ±è¨˜æ†¶é«”:")
    print(f"   ç¸½è¨ˆ: {memory.total / 1024**3:.1f} GB")
    print(f"   å·²ç”¨: {memory.used / 1024**3:.1f} GB")
    print(f"   å¯ç”¨: {memory.available / 1024**3:.1f} GB")
    print(f"   ä½¿ç”¨ç‡: {memory.percent:.1f}%")

    # GPU è¨˜æ†¶é«”
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            total = props.total_memory / 1024**3

            print(f"\nğŸ”¥ GPU {i} ({props.name}):")
            print(f"   ç¸½è¨ˆ: {total:.1f} GB")
            print(f"   å·²åˆ†é…: {allocated:.2f} GB")
            print(f"   å·²ä¿ç•™: {reserved:.2f} GB")
            print(f"   å¯ç”¨: {total - reserved:.2f} GB")
            print(f"   ä½¿ç”¨ç‡: {reserved/total*100:.1f}%")

            # è­¦å‘Š
            if reserved > total * 0.9:
                print(f"   âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜!")
            elif reserved > total * 0.7:
                print(f"   âš ï¸  æ³¨æ„: GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜")
            else:
                print(f"   âœ… GPU è¨˜æ†¶é«”ç‹€æ…‹è‰¯å¥½")
    else:
        print("\nâŒ æœªæª¢æ¸¬åˆ° GPU")

    print("=" * 60)


def estimate_model_memory():
    """ä¼°ç®— Breeze-ASR-25 æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚"""
    print("\nğŸ“Š Breeze-ASR-25 è¨˜æ†¶é«”éœ€æ±‚ä¼°ç®—")
    print("-" * 40)

    # æ¨¡å‹åƒæ•¸æ•¸é‡ (Breeze-ASR-25 ç´„ 244M åƒæ•¸)
    model_params = 244_000_000

    # FP32 è¨˜æ†¶é«”éœ€æ±‚
    fp32_memory = model_params * 4 / 1024**3  # 4 bytes per parameter
    print(f"æ¨¡å‹æ¬Šé‡ (FP32): {fp32_memory:.2f} GB")

    # æ¢¯åº¦è¨˜æ†¶é«”
    gradient_memory = fp32_memory
    print(f"æ¢¯åº¦ (FP32): {gradient_memory:.2f} GB")

    # AdamW å„ªåŒ–å™¨ç‹€æ…‹ (2x åƒæ•¸æ•¸é‡)
    optimizer_memory = model_params * 8 / 1024**3  # 8 bytes per parameter
    print(f"å„ªåŒ–å™¨ç‹€æ…‹ (AdamW): {optimizer_memory:.2f} GB")

    # æ¿€æ´»å€¼ä¼°ç®— (æ ¹æ“šæ‰¹æ¬¡å¤§å°)
    batch_sizes = [1, 2, 4, 8]
    for batch_size in batch_sizes:
        # ç²—ç•¥ä¼°ç®—: æ‰¹æ¬¡å¤§å° * åºåˆ—é•·åº¦ * éš±è—ç¶­åº¦ * å±¤æ•¸
        activation_memory = batch_size * 3000 * 1024 * 24 / 1024**3  # ç²—ç•¥ä¼°ç®—
        print(f"æ¿€æ´»å€¼ (batch_size={batch_size}): {activation_memory:.2f} GB")

    # ç¸½è¨ˆ
    base_memory = fp32_memory + gradient_memory + optimizer_memory
    print(f"\nåŸºç¤è¨˜æ†¶é«”éœ€æ±‚: {base_memory:.2f} GB")
    print(f"åŠ ä¸Šæ¿€æ´»å€¼ (batch_size=1): {base_memory + 0.7:.2f} GB")
    print(f"åŠ ä¸Šæ¿€æ´»å€¼ (batch_size=4): {base_memory + 2.8:.2f} GB")

    return base_memory


def test_memory_allocation():
    """æ¸¬è©¦è¨˜æ†¶é«”åˆ†é…"""
    print("\nğŸ§ª è¨˜æ†¶é«”åˆ†é…æ¸¬è©¦")
    print("-" * 30)

    if not torch.cuda.is_available():
        print("âŒ ç„¡æ³•æ¸¬è©¦ GPU è¨˜æ†¶é«”åˆ†é…")
        return

    # æ¸…ç†è¨˜æ†¶é«”
    torch.cuda.empty_cache()
    gc.collect()

    # æ¸¬è©¦ä¸åŒå¤§å°çš„å¼µé‡åˆ†é…
    sizes_mb = [100, 500, 1000, 2000, 5000]

    for size_mb in sizes_mb:
        try:
            # è¨ˆç®—å…ƒç´ æ•¸é‡
            elements = size_mb * 1024 * 1024 // 4  # 4 bytes per float32

            # å˜—è©¦åˆ†é…
            tensor = torch.zeros(elements, dtype=torch.float32, device="cuda")

            # æª¢æŸ¥å¯¦éš›åˆ†é…
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"âœ… æˆåŠŸåˆ†é… {size_mb} MB å¼µé‡ (å¯¦éš›: {allocated:.2f} GB)")

            # æ¸…ç†
            del tensor
            torch.cuda.empty_cache()

        except torch.cuda.OutOfMemoryError:
            print(f"âŒ ç„¡æ³•åˆ†é… {size_mb} MB å¼µé‡")
            break


def recommend_configuration():
    """æ ¹æ“šè¨˜æ†¶é«”ç‹€æ…‹æ¨è–¦é…ç½®"""
    print("\nğŸ¯ é…ç½®æ¨è–¦")
    print("-" * 20)

    if not torch.cuda.is_available():
        print("âŒ ç„¡ GPUï¼Œå»ºè­°ä½¿ç”¨ CPU æ¨¡å¼")
        return

    # ç²å– GPU è¨˜æ†¶é«”
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    gpu_name = torch.cuda.get_device_name(0)

    print(f"GPU: {gpu_name}")
    print(f"è¨˜æ†¶é«”: {total_memory:.1f} GB")

    if total_memory >= 40:
        print("âœ… A100 é…ç½®:")
        print("   batch_size: 4")
        print("   gradient_accumulation_steps: 4")
        print("   gradient_checkpointing: False")
        print("   max_steps: 4000")

    elif total_memory >= 30:
        print("âœ… V100 é…ç½®:")
        print("   batch_size: 2")
        print("   gradient_accumulation_steps: 8")
        print("   gradient_checkpointing: False")
        print("   max_steps: 2000")

    elif total_memory >= 20:
        print("âš ï¸  T4/P100 é…ç½®:")
        print("   batch_size: 1")
        print("   gradient_accumulation_steps: 16")
        print("   gradient_checkpointing: True")
        print("   max_steps: 1000")

    else:
        print("âŒ è¨˜æ†¶é«”ä¸è¶³ï¼Œå»ºè­°:")
        print("   1. å‡ç´šåˆ°æ›´å¤§è¨˜æ†¶é«”çš„ GPU")
        print("   2. ä½¿ç”¨æ¥µè‡´è¨˜æ†¶é«”æœ€ä½³åŒ–é…ç½®")
        print("   3. è€ƒæ…®ä½¿ç”¨ CPU æ¨¡å¼")


def cleanup_memory():
    """æ¸…ç†è¨˜æ†¶é«”"""
    print("\nğŸ§¹ è¨˜æ†¶é«”æ¸…ç†")
    print("-" * 20)

    # æ¸…ç† Python è¨˜æ†¶é«”
    gc.collect()

    # æ¸…ç† CUDA è¨˜æ†¶é«”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        # é‡è¨­çµ±è¨ˆ
        try:
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
        except:
            pass

    print("âœ… è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")


def main():
    """ä¸»è¨ºæ–·æµç¨‹"""
    print("ğŸ” Breeze-ASR-25 è¨˜æ†¶é«”è¨ºæ–·å·¥å…·")
    print("=" * 60)

    # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
    print_memory_status()

    # ä¼°ç®—æ¨¡å‹éœ€æ±‚
    estimate_model_memory()

    # æ¸¬è©¦è¨˜æ†¶é«”åˆ†é…
    test_memory_allocation()

    # æ¨è–¦é…ç½®
    recommend_configuration()

    # æ¸…ç†è¨˜æ†¶é«”
    cleanup_memory()

    # æœ€çµ‚ç‹€æ…‹
    print("\nğŸ“Š æ¸…ç†å¾Œç‹€æ…‹:")
    print_memory_status()


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Breeze-ASR-25 æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
å°ˆé–€ç”¨æ–¼ /standard ç›®éŒ„çš„æª”æ¡ˆä½ç½®
"""

import gc
import os
from dataclasses import dataclass
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import evaluate
import jiwer
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

# ==============================================================================
# åŸºæœ¬é…ç½® - æœ¬æ©Ÿç‰ˆæœ¬
# ==============================================================================

MODEL_ID = "MediaTek-Research/Breeze-ASR-25"
TRAIN_CSV = "metadata_train_fixed.csv"
TEST_CSV = "metadata_test_fixed.csv"
OUTPUT_DIR = "./breeze-asr-25-local"

# æœ¬æ©Ÿè¨“ç·´åƒæ•¸
QUICK_TEST_RATIO = 1  # å¿«é€Ÿæ¸¬è©¦ä½¿ç”¨ 10% è³‡æ–™
QUICK_MAX_STEPS = 5000  # è¨“ç·´æ­¥æ•¸

# ==============================================================================
# å…¨åŸŸå®šç¾© - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """è™•ç†èªéŸ³åˆ°åºåˆ—è³‡æ–™çš„ Data Collator - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""

    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:

        # 1. è™•ç† input_features (éŸ³è¨Šè²å­¸ç‰¹å¾µ)
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )
        # 2. è™•ç† labels (æ–‡æœ¬æ¨™ç±¤)
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # 3. è™•ç†æ¨™ç±¤ä¸­çš„ -100
        # å°‡å¡«å……éƒ¨åˆ†ï¼ˆattention_mask ç‚º 0ï¼‰çš„æ¨™ç±¤æ›¿æ›ç‚º -100ã€‚
        # åœ¨ PyTorch çš„äº¤å‰ç†µæå¤±å‡½æ•¸ä¸­ï¼Œ-100 æ˜¯ä¸€å€‹ç‰¹æ®Šçš„ ignore_indexï¼Œ
        # æ„å‘³è‘—é€™äº›ä½ç½®çš„æå¤±ä¸æœƒè¢«è¨ˆç®—ï¼Œé€™å°æ–¼å¡«å……éƒ¨åˆ†éå¸¸é‡è¦ã€‚
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # 4. è™•ç†é–‹é ­çš„ BOS token (å¦‚æœå­˜åœ¨)
        # æª¢æŸ¥æ‰€æœ‰åºåˆ—çš„ç¬¬ä¸€å€‹ token æ˜¯å¦éƒ½æ˜¯ BOS tokenã€‚
        # é€™æ˜¯é‡å°æŸäº›æ¨¡å‹ï¼ˆå¦‚ Whisperï¼‰çš„è¨“ç·´ç¿’æ…£ï¼Œæ¨¡å‹åœ¨è¨“ç·´æ™‚é€šå¸¸é æœŸæ¨™ç±¤ä¸åŒ…å«é–‹é ­çš„ BOS tokenã€‚
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch


def prepare_dataset_batched(batch, feature_extractor, tokenizer):
    """å°‡ä¸€æ‰¹éŸ³è¨Šå’Œæ–‡æœ¬è³‡æ–™å³æ™‚è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""
    audio_list = batch["audio"]
    batch["input_features"] = feature_extractor(
        [x["array"] for x in audio_list], sampling_rate=audio_list[0]["sampling_rate"]
    ).input_features
    batch["labels"] = tokenizer(
        batch["ä¸­æ–‡æ„è­¯"], max_length=448, truncation=True
    ).input_ids
    return batch


def compute_metrics(pred, tokenizer):
    """è¨ˆç®— WER æŒ‡æ¨™ - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    metric = evaluate.load("wer")
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}


# ==============================================================================
# æœ¬æ©Ÿç’°å¢ƒè¨­å®š
# ==============================================================================


def setup_local_environment():
    """è¨­å®šæœ¬æ©Ÿç’°å¢ƒ"""
    print("ğŸ”§ è¨­å®šæœ¬æ©Ÿç’°å¢ƒ...")

    # è¨­å®š CUDA è¨˜æ†¶é«”åˆ†é…ç­–ç•¥
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = (
        "expandable_segments:True,max_split_size_mb:128"
    )
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()
        print(f"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(
            f"âœ… GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )
    else:
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")

    print("âœ… æœ¬æ©Ÿç’°å¢ƒè¨­å®šå®Œæˆ")


# ==============================================================================
# è³‡æ–™è™•ç† - æœ¬æ©Ÿç‰ˆæœ¬
# ==============================================================================


def load_and_clean_data():
    """è¼‰å…¥ä¸¦æ¸…ç†è³‡æ–™ - æœ¬æ©Ÿç‰ˆæœ¬"""
    print("ğŸ“Š è¼‰å…¥è³‡æ–™...")

    if not Path(TRAIN_CSV).exists() or not Path(TEST_CSV).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {TRAIN_CSV} æˆ– {TEST_CSV}")

    train_df = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)

    required_cols = ["file", "ä¸­æ–‡æ„è­¯"]
    for col in required_cols:
        if col not in train_df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")

    # ç§»é™¤ç©ºå€¼
    train_df = train_df.dropna(subset=required_cols)
    test_df = test_df.dropna(subset=required_cols)

    # é™åˆ¶æ–‡å­—é•·åº¦
    train_df = train_df[train_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]
    test_df = test_df[test_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]

    # å¿«é€Ÿæ¸¬è©¦æ¡æ¨£
    train_size = len(train_df)
    test_size = len(test_df)

    train_sample_size = max(1, int(train_size * QUICK_TEST_RATIO))
    test_sample_size = max(1, int(test_size * QUICK_TEST_RATIO))

    train_df = train_df.sample(n=train_sample_size, random_state=42)
    test_df = test_df.sample(n=test_sample_size, random_state=42)

    # ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ - æœ¬æ©Ÿ /standard ç›®éŒ„
    print("ğŸ”§ ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘...")

    def fix_audio_path(path_str):
        if not isinstance(path_str, str):
            return path_str

        path_str = str(path_str).strip()

        # å¦‚æœå·²ç¶“æ˜¯æ­£ç¢ºè·¯å¾‘ï¼Œç›´æ¥è¿”å›
        if Path(path_str).exists():
            return path_str

        # è™•ç† Windows è·¯å¾‘
        if path_str.startswith(("C:", "D:", "E:", "F:")):
            path_parts = Path(path_str).parts
            try:
                # å°‹æ‰¾ standard ç›®éŒ„
                standard_idx = path_parts.index("standard")
                if standard_idx + 1 < len(path_parts):
                    relative_path = "/".join(path_parts[standard_idx:])
                    # ä½¿ç”¨æœ¬æ©Ÿ /standard ç›®éŒ„
                    local_path = f"/standard/{relative_path}"
                    if Path(local_path).exists():
                        return local_path
            except ValueError:
                pass

            # å¦‚æœæ‰¾ä¸åˆ° standardï¼Œä½¿ç”¨æª”æ¡ˆå
            filename = Path(path_str).name
            local_path = f"/standard/{filename}"
            if Path(local_path).exists():
                return local_path

        # è™•ç†ç›¸å°è·¯å¾‘
        if not path_str.startswith("/"):
            local_path = f"/standard/{path_str}"
            if Path(local_path).exists():
                return local_path

        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹è·¯å¾‘
        return path_str

    train_df["file"] = train_df["file"].apply(fix_audio_path)
    test_df["file"] = test_df["file"].apply(fix_audio_path)

    sample_path = train_df["file"].iloc[0] if len(train_df) > 0 else ""
    print(f"   ç¯„ä¾‹è·¯å¾‘: {sample_path}")

    print(f"âœ… åŸå§‹è¨“ç·´è³‡æ–™: {train_size} ç­†")
    print(f"âœ… åŸå§‹æ¸¬è©¦è³‡æ–™: {test_size} ç­†")
    print(f"âœ… æœ¬æ©Ÿå¿«é€Ÿæ¸¬è©¦è¨“ç·´è³‡æ–™: {len(train_df)} ç­† ({QUICK_TEST_RATIO*100}%)")
    print(f"âœ… æœ¬æ©Ÿå¿«é€Ÿæ¸¬è©¦æ¸¬è©¦è³‡æ–™: {len(test_df)} ç­† ({QUICK_TEST_RATIO*100}%)")

    return train_df, test_df


# ==============================================================================
# ä¸»è¨“ç·´å‡½æ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


def main():
    """ä¸»è¨“ç·´æµç¨‹ - æœ¬æ©Ÿç‰ˆæœ¬"""
    print("ğŸš€ Breeze-ASR-25 æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬é–‹å§‹")
    print("=" * 60)
    print(f"ğŸ¯ ä½¿ç”¨è³‡æ–™æ¯”ä¾‹: {QUICK_TEST_RATIO*100}%")
    print(f"ğŸ¯ æœ€å¤§è¨“ç·´æ­¥æ•¸: {QUICK_MAX_STEPS}")

    setup_local_environment()

    # è¼‰å…¥è³‡æ–™
    train_df, test_df = load_and_clean_data()

    print("ğŸ¤– è¼‰å…¥æ¨¡å‹...")
    processor = WhisperProcessor.from_pretrained(MODEL_ID)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)

    # è¨­å®šæ¨¡å‹é…ç½® - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    print("ğŸ“‹ å‰µå»ºè³‡æ–™é›†...")

    # å‰µå»ºè³‡æ–™é›† - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    # è½‰æ›ç‚ºéŸ³é »æ ¼å¼
    train_dataset = train_dataset.cast_column("file", Audio(sampling_rate=16000))
    test_dataset = test_dataset.cast_column("file", Audio(sampling_rate=16000))

    # é‡å‘½åæ¬„ä½
    train_dataset = train_dataset.rename_column("file", "audio")
    test_dataset = test_dataset.rename_column("file", "audio")

    # ä½¿ç”¨ .with_transform() ç¢ºä¿è¨˜æ†¶é«”ç©©å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    prepare_fn = partial(
        prepare_dataset_batched,
        feature_extractor=processor.feature_extractor,
        tokenizer=processor.tokenizer,
    )

    train_dataset = train_dataset.with_transform(prepare_fn)
    test_dataset = test_dataset.with_transform(prepare_fn)

    print("âœ… å³æ™‚è½‰æ›å·²è¨­å®š")

    # å»ºç«‹è¨“ç·´å…ƒä»¶ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    print("ğŸ”§ å»ºç«‹è¨“ç·´å…ƒä»¶...")
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)

    # è¨“ç·´åƒæ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼ï¼Œé©åˆæœ¬æ©Ÿç’°å¢ƒ
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # æ‰¹æ¬¡å¤§å°è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        per_device_train_batch_size=2,  # ä½¿ç”¨ train.py çš„è¨­å®š
        per_device_eval_batch_size=2,  # ä½¿ç”¨ train.py çš„è¨­å®š
        gradient_accumulation_steps=8,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # å­¸ç¿’ç‡å’Œæ­¥æ•¸è¨­å®š
        learning_rate=1e-5,
        warmup_steps=500,
        max_steps=QUICK_MAX_STEPS,
        # è©•ä¼°è¨­å®š
        eval_strategy="steps",
        eval_steps=500,
        predict_with_generate=True,
        generation_max_length=64,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # ä¿å­˜è¨­å®š
        save_steps=500,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        # å„ªåŒ–è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        gradient_checkpointing=False,
        fp16=False,  # ä½¿ç”¨ train.py çš„è¨­å®š
        dataloader_num_workers=0,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # å…¶ä»–è¨­å®š
        logging_steps=250,
        report_to=[],
        remove_unused_columns=False,
        push_to_hub=False,  # æœ¬æ©Ÿç‰ˆæœ¬ä¸ä¸Šå‚³
    )

    # å‰µå»ºè¨“ç·´å™¨ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics_fn,
        tokenizer=processor.feature_extractor,
    )

    # é¡¯ç¤ºè¨“ç·´è³‡è¨Š
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    effective_batch = (
        training_args.per_device_train_batch_size
        * training_args.gradient_accumulation_steps
    )

    print(f"\nğŸ“ˆ æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬è³‡è¨Š:")
    print(f"   æ¨¡å‹åƒæ•¸: {total_params:,}")
    print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
    print(f"   æœ€å¤§æ­¥æ•¸: {QUICK_MAX_STEPS}")
    print(f"   å­¸ç¿’ç‡: {training_args.learning_rate}")
    print(f"   è©•ä¼°é »ç‡: æ¯ {training_args.eval_steps} æ­¥")
    print(f"   ä½¿ç”¨ train.py æˆåŠŸæ¨¡å¼")
    print(f"   æª”æ¡ˆä½ç½®: /standard ç›®éŒ„")

    print("\nğŸš€ é–‹å§‹æœ¬æ©Ÿè¨“ç·´...")
    try:
        trainer.train()
        print("âœ… æœ¬æ©Ÿè¨“ç·´å®Œæˆ")

        # ä¿å­˜æ¨¡å‹
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)

        # æœ€çµ‚è©•ä¼°
        metrics = trainer.evaluate()
        print(f"ğŸ¯ æœ€çµ‚ WER: {metrics.get('eval_wer', 'N/A'):.2f}%")

        print(f"\nğŸ’¾ æœ¬æ©Ÿè¨“ç·´æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")

    except Exception as e:
        print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()

    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()


if __name__ == "__main__":
    main()

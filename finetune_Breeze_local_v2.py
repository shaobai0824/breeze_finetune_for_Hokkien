#!/usr/bin/env python3
"""
Breeze-ASR-25 æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
å°ˆé–€ç”¨æ–¼ /standard ç›®éŒ„çš„æª”æ¡ˆä½ç½®
"""

import gc
import os
from dataclasses import dataclass
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import evaluate
import jiwer
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

from evaluate_with_semantic_similarity import (
    SemanticSimilarityEvaluator,
)  # å°å…¥èªç¾©ç›¸ä¼¼åº¦è©•ä¼°å™¨

# ==============================================================================
# åŸºæœ¬é…ç½® - æœ¬æ©Ÿç‰ˆæœ¬
# ==============================================================================

MODEL_ID = "shaobai880824/breeze-asr-25-local-hokkien_v1"
DATA_CSV = (
    "audio_1_converted_merged_zh.csv"  # <--- ä¿®æ”¹é€™è£¡ï¼ŒæŒ‡å®šæ‚¨åŒ…å«æ‰€æœ‰è³‡æ–™çš„ CSV æª”æ¡ˆ
)
TEST_SPLIT_RATIO = 0.2  # æ¸¬è©¦é›†ä½”ç¸½è³‡æ–™çš„æ¯”ä¾‹ (ä¾‹å¦‚ 0.2 è¡¨ç¤º 20%)
OUTPUT_DIR = "./breeze-asr-25-local-hokkien_v2"

# æœ¬æ©Ÿè¨“ç·´åƒæ•¸
QUICK_TEST_RATIO = 1  # å¿«é€Ÿæ¸¬è©¦ä½¿ç”¨ 10% è³‡æ–™
QUICK_MAX_STEPS = 500  # è¨“ç·´æ­¥æ•¸

# ==============================================================================
# å…¨åŸŸå®šç¾© - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================

# ç‚ºäº†æ•ˆç‡ï¼Œåœ¨æ­¤è™•åˆå§‹åŒ–è©•ä¼°å™¨ï¼Œä½¿å…¶åœ¨æ•´å€‹è¨“ç·´éç¨‹ä¸­åƒ…è¼‰å…¥ä¸€æ¬¡
print("â³ æ­£åœ¨é å…ˆè¼‰å…¥èªç¾©ç›¸ä¼¼åº¦è©•ä¼°æ¨¡å‹...")
semantic_evaluator = SemanticSimilarityEvaluator("shibing624/text2vec-base-chinese")
print("âœ… èªç¾©ç›¸ä¼¼åº¦è©•ä¼°æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """è™•ç†èªéŸ³åˆ°åºåˆ—è³‡æ–™çš„ Data Collator - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""

    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:

        # 1. è™•ç† input_features (éŸ³è¨Šè²å­¸ç‰¹å¾µ)
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )
        # 2. è™•ç† labels (æ–‡æœ¬æ¨™ç±¤)
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # 3. è™•ç†æ¨™ç±¤ä¸­çš„ -100
        # å°‡å¡«å……éƒ¨åˆ†ï¼ˆattention_mask ç‚º 0ï¼‰çš„æ¨™ç±¤æ›¿æ›ç‚º -100ã€‚
        # åœ¨ PyTorch çš„äº¤å‰ç†µæå¤±å‡½æ•¸ä¸­ï¼Œ-100 æ˜¯ä¸€å€‹ç‰¹æ®Šçš„ ignore_indexï¼Œ
        # æ„å‘³è‘—é€™äº›ä½ç½®çš„æå¤±ä¸æœƒè¢«è¨ˆç®—ï¼Œé€™å°æ–¼å¡«å……éƒ¨åˆ†éå¸¸é‡è¦ã€‚
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # 4. è™•ç†é–‹é ­çš„ BOS token (å¦‚æœå­˜åœ¨)
        # æª¢æŸ¥æ‰€æœ‰åºåˆ—çš„ç¬¬ä¸€å€‹ token æ˜¯å¦éƒ½æ˜¯ BOS tokenã€‚
        # é€™æ˜¯é‡å°æŸäº›æ¨¡å‹ï¼ˆå¦‚ Whisperï¼‰çš„è¨“ç·´ç¿’æ…£ï¼Œæ¨¡å‹åœ¨è¨“ç·´æ™‚é€šå¸¸é æœŸæ¨™ç±¤ä¸åŒ…å«é–‹é ­çš„ BOS tokenã€‚
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch


def prepare_dataset_batched(batch, feature_extractor, tokenizer):
    """å°‡ä¸€æ‰¹éŸ³è¨Šå’Œæ–‡æœ¬è³‡æ–™å³æ™‚è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""
    audio_list = batch["audio"]
    batch["input_features"] = feature_extractor(
        # ## ä¿®æ”¹é»: é€™è£¡ä¸å†éœ€è¦æ“”å¿ƒ audio_list[0] æ˜¯ Noneï¼Œå› ç‚ºå£è³‡æ–™å·²è¢«éæ¿¾
        [x["array"] for x in audio_list],
        sampling_rate=audio_list[0]["sampling_rate"],
    ).input_features
    batch["labels"] = tokenizer(
        batch["æ¼¢å­—"], max_length=448, truncation=True
    ).input_ids
    return batch


def compute_metrics(pred, tokenizer):
    """è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦æŒ‡æ¨™"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # å°‡ -100 æ›¿æ›ç‚º pad_token_id
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # è§£ç¢¼é æ¸¬å’Œæ¨™ç±¤
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # ä½¿ç”¨å…¨åŸŸçš„ semantic_evaluator è¨ˆç®—ç›¸ä¼¼åº¦
    similarities = [
        semantic_evaluator.compute_similarity(ref, pred)
        for ref, pred in zip(label_str, pred_str)
    ]

    # è¨ˆç®—å¹³å‡ç›¸ä¼¼åº¦
    avg_similarity = np.mean(similarities) if similarities else 0.0

    return {"semantic_similarity": avg_similarity}


# ==============================================================================
# æœ¬æ©Ÿç’°å¢ƒè¨­å®š
# ==============================================================================


def setup_local_environment():
    """è¨­å®šæœ¬æ©Ÿç’°å¢ƒ"""
    print("ğŸ”§ è¨­å®šæœ¬æ©Ÿç’°å¢ƒ...")

    # è¨­å®š CUDA è¨˜æ†¶é«”åˆ†é…ç­–ç•¥
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = (
        "expandable_segments:True,max_split_size_mb:128"
    )
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()
        print(f"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(
            f"âœ… GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )
    else:
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")

    print("âœ… æœ¬æ©Ÿç’°å¢ƒè¨­å®šå®Œæˆ")


# ==============================================================================
# è³‡æ–™è™•ç† - æœ¬æ©Ÿç‰ˆæœ¬
# ==============================================================================


def load_and_clean_data():
    """è¼‰å…¥ã€æ¸…ç†ä¸¦å¾å–®ä¸€æª”æ¡ˆæ‹†åˆ†è³‡æ–™ - æœ¬æ©Ÿç‰ˆæœ¬"""
    print("ğŸ“Š è¼‰å…¥ã€æ¸…ç†ä¸¦æ‹†åˆ†è³‡æ–™...")

    if not Path(DATA_CSV).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æŒ‡å®šçš„è³‡æ–™æª”æ¡ˆ: {DATA_CSV}")

    df = pd.read_csv(DATA_CSV)

    required_cols = ["æª”æ¡ˆä½ç½®", "æ¼¢å­—"]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")

    # åœ¨æ‹†åˆ†å‰å°æ•´å€‹è³‡æ–™é›†é€²è¡Œæ¸…ç†
    df.dropna(subset=required_cols, inplace=True)
    df = df[df["æ¼¢å­—"].str.len() < 200].copy()

    print(f"âœ… æ¸…ç†å¾Œç¸½è³‡æ–™: {len(df)} ç­†")

    # æ ¹æ“šæ¯”ä¾‹æ‹†åˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    test_df = df.sample(frac=TEST_SPLIT_RATIO, random_state=42)
    train_df = df.drop(test_df.index)

    train_size_after_split = len(train_df)
    test_size_after_split = len(test_df)

    print(f"\nâœ… æ‹†åˆ†å¾Œè¨“ç·´è³‡æ–™: {train_size_after_split} ç­†")
    print(
        f"âœ… æ‹†åˆ†å¾Œæ¸¬è©¦è³‡æ–™: {test_size_after_split} ç­† (ç›®æ¨™æ¯”ä¾‹: {TEST_SPLIT_RATIO*100:.1f}%)"
    )

    # åŸºæ–¼æ‹†åˆ†å¾Œçš„è³‡æ–™ï¼Œå†é€²è¡Œå¿«é€Ÿæ¸¬è©¦çš„æ¡æ¨£
    if QUICK_TEST_RATIO < 1.0:
        train_df = train_df.sample(frac=QUICK_TEST_RATIO, random_state=42)
        test_df = test_df.sample(frac=QUICK_TEST_RATIO, random_state=42)
        print(f"âœ… å¿«é€Ÿæ¸¬è©¦æ¨¡å¼å·²å•Ÿç”¨ (ä½¿ç”¨ {QUICK_TEST_RATIO*100}%)")
        print(f"  - ä½¿ç”¨è¨“ç·´è³‡æ–™: {len(train_df)} ç­†")
        print(f"  - ä½¿ç”¨æ¸¬è©¦è³‡æ–™: {len(test_df)} ç­†")

    # ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ - æœ¬æ©Ÿ /standard ç›®éŒ„
    print("ğŸ”§ ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘...")

    def fix_audio_path(path_str):
        if not isinstance(path_str, str):
            return path_str

        path_str = str(path_str).strip()

        # å¦‚æœå·²ç¶“æ˜¯æ­£ç¢ºè·¯å¾‘ï¼Œç›´æ¥è¿”å›
        if Path(path_str).exists():
            return path_str

        # è™•ç† Windows è·¯å¾‘
        if path_str.startswith(("C:", "D:", "E:", "F:")):
            path_parts = Path(path_str).parts
            try:
                # å°‹æ‰¾ standard ç›®éŒ„
                standard_idx = path_parts.index("standard")
                if standard_idx + 1 < len(path_parts):
                    relative_path = "/".join(path_parts[standard_idx:])
                    # ä½¿ç”¨æœ¬æ©Ÿ /standard ç›®éŒ„
                    local_path = f"/standard/{relative_path}"
                    if Path(local_path).exists():
                        return local_path
            except ValueError:
                pass

            # å¦‚æœæ‰¾ä¸åˆ° standardï¼Œä½¿ç”¨æª”æ¡ˆå
            filename = Path(path_str).name
            local_path = f"/standard/{filename}"
            if Path(local_path).exists():
                return local_path

        # è™•ç†ç›¸å°è·¯å¾‘
        if not path_str.startswith("/"):
            local_path = f"/standard/{path_str}"
            if Path(local_path).exists():
                return local_path

        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹è·¯å¾‘
        return path_str

    train_df.loc[:, "æª”æ¡ˆä½ç½®"] = train_df["æª”æ¡ˆä½ç½®"].apply(fix_audio_path)
    test_df.loc[:, "æª”æ¡ˆä½ç½®"] = test_df["æª”æ¡ˆä½ç½®"].apply(fix_audio_path)

    sample_path = train_df["æª”æ¡ˆä½ç½®"].iloc[0] if len(train_df) > 0 else ""
    print(f"  ç¯„ä¾‹è·¯å¾‘: {sample_path}")

    return train_df, test_df


# ==============================================================================
# ä¸»è¨“ç·´å‡½æ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


def main():
    """ä¸»è¨“ç·´æµç¨‹ - æœ¬æ©Ÿç‰ˆæœ¬"""
    print("ğŸš€ Breeze-ASR-25 æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬é–‹å§‹")
    print("=" * 60)
    print(f"ğŸ¯ ä½¿ç”¨è³‡æ–™æ¯”ä¾‹: {QUICK_TEST_RATIO*100}%")
    print(f"ğŸ¯ æœ€å¤§è¨“ç·´æ­¥æ•¸: {QUICK_MAX_STEPS}")

    setup_local_environment()

    # è¼‰å…¥è³‡æ–™
    train_df, test_df = load_and_clean_data()

    print("ğŸ¤– è¼‰å…¥æ¨¡å‹...")
    processor = WhisperProcessor.from_pretrained(MODEL_ID)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)

    # è¨­å®šæ¨¡å‹é…ç½® - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    print("ğŸ“‹ å‰µå»ºè³‡æ–™é›†...")

    # å‰µå»ºè³‡æ–™é›† - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    # ## ==================== ä¿®æ”¹é» START ==================== ##
    # ## ä¿®æ­£æ¬„ä½åç¨±ä¸¦åŠ å…¥è³‡æ–™éæ¿¾æ­¥é©Ÿä¾†æ ¹é™¤ 'NoneType' éŒ¯èª¤

    print("ğŸ”Š è½‰æ›éŸ³è¨Šæ ¼å¼ä¸¦é©—è­‰è³‡æ–™...")
    # 1. è½‰æ›ç‚ºéŸ³é »æ ¼å¼ (ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½å "æª”æ¡ˆä½ç½®")
    #    é€™ä¸€æ­¥æœƒå°‡ç„¡æ³•è®€å–çš„éŸ³æª”è·¯å¾‘è½‰æ›ç‚º None
    train_dataset = train_dataset.cast_column("æª”æ¡ˆä½ç½®", Audio(sampling_rate=16000))
    test_dataset = test_dataset.cast_column("æª”æ¡ˆä½ç½®", Audio(sampling_rate=16000))

    # 2. é‡å‘½åæ¬„ä½ä»¥ç¬¦åˆå¾ŒçºŒè™•ç†
    train_dataset = train_dataset.rename_column("æª”æ¡ˆä½ç½®", "audio")
    test_dataset = test_dataset.rename_column("æª”æ¡ˆä½ç½®", "audio")

    # 3. éæ¿¾æ‰å€¼ç‚º None çš„ç„¡æ•ˆéŸ³è¨Šè³‡æ–™ (é—œéµæ­¥é©Ÿ)
    def is_audio_valid(example):
        return example["audio"] is not None

    original_train_len = len(train_dataset)
    original_test_len = len(test_dataset)

    train_dataset = train_dataset.filter(is_audio_valid, num_proc=4)  # å¯èª¿æ•´ num_proc
    test_dataset = test_dataset.filter(is_audio_valid, num_proc=4)

    filtered_train_len = len(train_dataset)
    filtered_test_len = len(test_dataset)

    if original_train_len != filtered_train_len:
        print(
            f"ğŸ§¹ å¾è¨“ç·´é›†ä¸­ç§»é™¤äº† {original_train_len - filtered_train_len} ç­†ç„¡æ•ˆéŸ³è¨Šã€‚"
        )
    if original_test_len != filtered_test_len:
        print(
            f"ğŸ§¹ å¾æ¸¬è©¦é›†ä¸­ç§»é™¤äº† {original_test_len - filtered_test_len} ç­†ç„¡æ•ˆéŸ³è¨Šã€‚"
        )

    print(f"âœ… æœ‰æ•ˆè¨“ç·´è³‡æ–™: {filtered_train_len} ç­†")
    print(f"âœ… æœ‰æ•ˆæ¸¬è©¦è³‡æ–™: {filtered_test_len} ç­†")

    # ## ===================== ä¿®æ”¹é» END ===================== ##

    # ä½¿ç”¨ .with_transform() ç¢ºä¿è¨˜æ†¶é«”ç©©å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    prepare_fn = partial(
        prepare_dataset_batched,
        feature_extractor=processor.feature_extractor,
        tokenizer=processor.tokenizer,
    )

    # ## ä¿®æ”¹é»: ä½¿ç”¨å·²éæ¿¾çš„ dataset
    train_dataset = train_dataset.with_transform(prepare_fn)
    test_dataset = test_dataset.with_transform(prepare_fn)

    print("âœ… å³æ™‚è½‰æ›å·²è¨­å®š")

    # å»ºç«‹è¨“ç·´å…ƒä»¶ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    print("ğŸ”§ å»ºç«‹è¨“ç·´å…ƒä»¶...")
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)

    # è¨“ç·´åƒæ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼ï¼Œé©åˆæœ¬æ©Ÿç’°å¢ƒ
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # æ‰¹æ¬¡å¤§å°è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        per_device_train_batch_size=1,  # ä½¿ç”¨ train.py çš„è¨­å®š
        per_device_eval_batch_size=1,  # ä½¿ç”¨ train.py çš„è¨­å®š
        gradient_accumulation_steps=16,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # å­¸ç¿’ç‡å’Œæ­¥æ•¸è¨­å®š
        learning_rate=1e-5,
        warmup_steps=100,
        max_steps=QUICK_MAX_STEPS,
        # è©•ä¼°è¨­å®š
        eval_strategy="steps",
        eval_steps=100,
        predict_with_generate=True,
        generation_max_length=64,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # ä¿å­˜è¨­å®š
        save_steps=100,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="semantic_similarity",  # <-- æ”¹ç‚ºèªç¾©ç›¸ä¼¼åº¦
        greater_is_better=True,  # <-- åˆ†æ•¸è¶Šé«˜è¶Šå¥½
        # å„ªåŒ–è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        gradient_checkpointing=False,
        fp16=False,  # ä½¿ç”¨ train.py çš„è¨­å®š
        dataloader_num_workers=0,  # ä½¿ç”¨ train.py çš„è¨­å®š
        # å…¶ä»–è¨­å®š
        logging_steps=10,
        report_to=[],
        remove_unused_columns=False,
        push_to_hub=True,  # ## ä¿®æ”¹é»ï¼šæœ¬æ©Ÿç‰ˆæœ¬é è¨­ä¸ä¸Šå‚³åˆ°Hugging Face Hub
    )

    # å‰µå»ºè¨“ç·´å™¨ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,  # ## ä¿®æ”¹é»: ä½¿ç”¨å·²éæ¿¾çš„ dataset
        eval_dataset=test_dataset,  # ## ä¿®æ”¹é»: ä½¿ç”¨å·²éæ¿¾çš„ dataset
        data_collator=data_collator,
        compute_metrics=compute_metrics_fn,
        tokenizer=processor.tokenizer,  # <-- ä¿®æ­£ï¼šæ‡‰å‚³é tokenizer
    )

    # é¡¯ç¤ºè¨“ç·´è³‡è¨Š
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    effective_batch = (
        training_args.per_device_train_batch_size
        * training_args.gradient_accumulation_steps
    )

    print(f"\nğŸ“ˆ æœ¬æ©Ÿè¨“ç·´ç‰ˆæœ¬è³‡è¨Š:")
    print(f"  æ¨¡å‹åƒæ•¸: {total_params:,}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
    print(f"  æœ€å¤§æ­¥æ•¸: {QUICK_MAX_STEPS}")
    print(f"  å­¸ç¿’ç‡: {training_args.learning_rate}")
    print(f"  è©•ä¼°é »ç‡: æ¯ {training_args.eval_steps} æ­¥")
    print(f"  ä½¿ç”¨ train.py æˆåŠŸæ¨¡å¼")
    print(f"  æª”æ¡ˆä½ç½®: /standard ç›®éŒ„")

    print("\nğŸš€ é–‹å§‹æœ¬æ©Ÿè¨“ç·´...")
    try:
        trainer.train()
        print("âœ… æœ¬æ©Ÿè¨“ç·´å®Œæˆ")

        # ä¿å­˜æ¨¡å‹
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)

        # æœ€çµ‚è©•ä¼°
        metrics = trainer.evaluate()
        final_metric_name = f"eval_{training_args.metric_for_best_model}"
        print(
            f"ğŸ¯ æœ€çµ‚ {training_args.metric_for_best_model}: {metrics.get(final_metric_name, 'N/A'):.4f}"
        )

        print(f"\nğŸ’¾ æœ¬æ©Ÿè¨“ç·´æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")

    except Exception as e:
        print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()

    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()


if __name__ == "__main__":
    main()

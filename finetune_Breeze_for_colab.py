#!/usr/bin/env python3
"""
Breeze-ASR-25 æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
ä½¿ç”¨æ­£ç¢ºçš„è³‡æ–™è™•ç†æ–¹å¼å’Œè¨“ç·´åƒæ•¸
"""

import gc
import os
from dataclasses import dataclass
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import evaluate
import jiwer
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

# ==============================================================================
# åŸºæœ¬é…ç½®
# ==============================================================================

MODEL_ID = "MediaTek-Research/Breeze-ASR-25"
TRAIN_CSV = "metadata_train.csv"
TEST_CSV = "metadata_test.csv"
OUTPUT_DIR = "./breeze-asr-25-final-corrected"

# ä½¿ç”¨ train.py çš„æˆåŠŸåƒæ•¸
QUICK_TEST_RATIO = 1  # å¿«é€Ÿæ¸¬è©¦ä½¿ç”¨ 10% è³‡æ–™
QUICK_MAX_STEPS = 500  # å¢åŠ æ­¥æ•¸

# ==============================================================================
# å…¨åŸŸå®šç¾© - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """è™•ç†èªéŸ³åˆ°åºåˆ—è³‡æ–™çš„ Data Collator - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""

    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch


def prepare_dataset_batched(batch, feature_extractor, tokenizer):
    """å°‡ä¸€æ‰¹éŸ³è¨Šå’Œæ–‡æœ¬è³‡æ–™å³æ™‚è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""
    audio_list = batch["audio"]
    batch["input_features"] = feature_extractor(
        [x["array"] for x in audio_list], sampling_rate=audio_list[0]["sampling_rate"]
    ).input_features
    batch["labels"] = tokenizer(
        batch["ä¸­æ–‡æ„è­¯"], max_length=448, truncation=True
    ).input_ids
    return batch


def compute_metrics(pred, tokenizer):
    """è¨ˆç®— WER æŒ‡æ¨™ - åŸºæ–¼ train.py æˆåŠŸç‰ˆæœ¬"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    metric = evaluate.load("wer")
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}


# ==============================================================================
# Colab ç’°å¢ƒè¨­å®š
# ==============================================================================


def setup_colab_environment():
    """è¨­å®š Colab ç’°å¢ƒ"""
    print("ğŸ”§ è¨­å®š Colab ç’°å¢ƒ...")

    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = (
        "expandable_segments:True,max_split_size_mb:128"
    )
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

    try:
        import sys

        if "google.colab" in sys.modules:
            from google.colab import drive

            drive.mount("/content/drive")
            print("âœ… Google Drive å·²æ›è¼‰")
        else:
            print("â„¹ï¸  é Colab ç’°å¢ƒï¼Œè·³é Drive æ›è¼‰")
    except ImportError:
        print("â„¹ï¸  ç„¡æ³•æ›è¼‰ Google Drive (å¯èƒ½å·²æ›è¼‰)")

    print("âœ… Colab ç’°å¢ƒè¨­å®šå®Œæˆ")


# ==============================================================================
# è³‡æ–™è™•ç† - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


def load_and_clean_data():
    """è¼‰å…¥ä¸¦æ¸…ç†è³‡æ–™ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼"""
    print("ğŸ“Š è¼‰å…¥è³‡æ–™...")

    if not Path(TRAIN_CSV).exists() or not Path(TEST_CSV).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {TRAIN_CSV} æˆ– {TEST_CSV}")

    train_df = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)

    required_cols = ["file", "ä¸­æ–‡æ„è­¯"]
    for col in required_cols:
        if col not in train_df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")

    # ç§»é™¤ç©ºå€¼
    train_df = train_df.dropna(subset=required_cols)
    test_df = test_df.dropna(subset=required_cols)

    # é™åˆ¶æ–‡å­—é•·åº¦
    train_df = train_df[train_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]
    test_df = test_df[test_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]

    # å¿«é€Ÿæ¸¬è©¦æ¡æ¨£
    train_size = len(train_df)
    test_size = len(test_df)

    train_sample_size = max(1, int(train_size * QUICK_TEST_RATIO))
    test_sample_size = max(1, int(test_size * QUICK_TEST_RATIO))

    train_df = train_df.sample(n=train_sample_size, random_state=42)
    test_df = test_df.sample(n=test_sample_size, random_state=42)

    # ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ - ä½¿ç”¨åŸæœ¬çš„ä½ç½®
    print("ğŸ”§ ä¿®æ­£éŸ³è¨Šæª”æ¡ˆè·¯å¾‘...")

    def fix_audio_path(path_str):
        if not isinstance(path_str, str):
            return path_str

        path_str = str(path_str).strip()

        # å¦‚æœå·²ç¶“æ˜¯ Google Drive è·¯å¾‘ï¼Œç›´æ¥è¿”å›
        if path_str.startswith("/content/drive/MyDrive/"):
            return path_str

        # è™•ç† Windows è·¯å¾‘
        if path_str.startswith(("C:", "D:", "E:", "F:")):
            path_parts = Path(path_str).parts
            try:
                # å°‹æ‰¾ standard ç›®éŒ„
                standard_idx = path_parts.index("standard")
                if standard_idx + 1 < len(path_parts):
                    relative_path = "/".join(path_parts[standard_idx:])
                    return f"/content/drive/MyDrive/{relative_path}"
            except ValueError:
                pass

            # å¦‚æœæ‰¾ä¸åˆ° standardï¼Œä½¿ç”¨æª”æ¡ˆå
            filename = Path(path_str).name
            return f"/content/drive/MyDrive/{filename}"

        # è™•ç†ç›¸å°è·¯å¾‘
        if not path_str.startswith("/"):
            return f"/content/drive/MyDrive/{path_str}"

        return path_str

    train_df["file"] = train_df["file"].apply(fix_audio_path)
    test_df["file"] = test_df["file"].apply(fix_audio_path)

    sample_path = train_df["file"].iloc[0] if len(train_df) > 0 else ""
    print(f"   ç¯„ä¾‹è·¯å¾‘: {sample_path}")

    print(f"âœ… åŸå§‹è¨“ç·´è³‡æ–™: {train_size} ç­†")
    print(f"âœ… åŸå§‹æ¸¬è©¦è³‡æ–™: {test_size} ç­†")
    print(f"âœ… å¿«é€Ÿæ¸¬è©¦è¨“ç·´è³‡æ–™: {len(train_df)} ç­† ({QUICK_TEST_RATIO*100}%)")
    print(f"âœ… å¿«é€Ÿæ¸¬è©¦æ¸¬è©¦è³‡æ–™: {len(test_df)} ç­† ({QUICK_TEST_RATIO*100}%)")

    return train_df, test_df


# ==============================================================================
# ä¸»è¨“ç·´å‡½æ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
# ==============================================================================


def main():
    """ä¸»è¨“ç·´æµç¨‹ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼"""
    print("ğŸš€ Breeze-ASR-25 æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬é–‹å§‹")
    print("=" * 60)
    print(f"ğŸ¯ ä½¿ç”¨è³‡æ–™æ¯”ä¾‹: {QUICK_TEST_RATIO*100}%")
    print(f"ğŸ¯ æœ€å¤§è¨“ç·´æ­¥æ•¸: {QUICK_MAX_STEPS}")

    setup_colab_environment()

    # è¼‰å…¥è³‡æ–™
    train_df, test_df = load_and_clean_data()

    print("ğŸ¤– è¼‰å…¥æ¨¡å‹...")
    processor = WhisperProcessor.from_pretrained(MODEL_ID)
    model = WhisperForConditionalGeneration.from_pretrained(
        MODEL_ID,
        # torch_dtype=torch.float16,  # ä½¿ç”¨ FP16 è¼‰å…¥
        low_cpu_mem_usage=True,  # ä½ CPU è¨˜æ†¶é«”ä½¿ç”¨
        use_cache=False,  # é—œé–‰å¿«å–
    )

    # è¨­å®šæ¨¡å‹é…ç½® - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    print("ğŸ“‹ å‰µå»ºè³‡æ–™é›†...")

    # å‰µå»ºè³‡æ–™é›† - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    # è½‰æ›ç‚ºéŸ³é »æ ¼å¼
    train_dataset = train_dataset.cast_column("file", Audio(sampling_rate=16000))
    test_dataset = test_dataset.cast_column("file", Audio(sampling_rate=16000))

    # é‡å‘½åæ¬„ä½
    train_dataset = train_dataset.rename_column("file", "audio")
    test_dataset = test_dataset.rename_column("file", "audio")

    # ä½¿ç”¨ .with_transform() ç¢ºä¿è¨˜æ†¶é«”ç©©å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    prepare_fn = partial(
        prepare_dataset_batched,
        feature_extractor=processor.feature_extractor,
        tokenizer=processor.tokenizer,
    )

    train_dataset = train_dataset.with_transform(prepare_fn)
    test_dataset = test_dataset.with_transform(prepare_fn)

    print("âœ… å³æ™‚è½‰æ›å·²è¨­å®š")

    # å»ºç«‹è¨“ç·´å…ƒä»¶ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    print("ğŸ”§ å»ºç«‹è¨“ç·´å…ƒä»¶...")
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)

    # è¨“ç·´åƒæ•¸ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼ä½†èª¿æ•´ç‚º Breeze é©ç”¨
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # æ‰¹æ¬¡å¤§å°è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        per_device_train_batch_size=2,  # å¾ 4 é™åˆ° 1
        per_device_eval_batch_size=2,  # å¾ 4 é™åˆ° 1
        gradient_accumulation_steps=8,  # å¾ 4 å¢åŠ åˆ° 8 (ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°)
        # å­¸ç¿’ç‡å’Œæ­¥æ•¸è¨­å®š
        learning_rate=1e-5,
        warmup_steps=100,
        max_steps=QUICK_MAX_STEPS,
        # è©•ä¼°è¨­å®š
        eval_strategy="steps",
        eval_steps=100,
        predict_with_generate=True,
        generation_max_length=64,  # æ¸›å°‘ç”Ÿæˆé•·åº¦ï¼Œé™ä½è¨˜æ†¶é«”ä½¿ç”¨
        # ä¿å­˜è¨­å®š
        save_steps=200,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        # å„ªåŒ–è¨­å®š - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
        gradient_checkpointing=False,  # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼Œç”¨è¨ˆç®—æ™‚é–“æ›å–è¨˜æ†¶é«”
        fp16=False,  # å•Ÿç”¨ FP16 å¯ä»¥æ¸›å°‘ç´„ 50% è¨˜æ†¶é«”ä½¿ç”¨
        dataloader_num_workers=0,  # é—œé–‰å¤šé€²ç¨‹è¼‰å…¥ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        # å…¶ä»–è¨­å®š
        logging_steps=25,
        report_to=[],
        remove_unused_columns=False,
        push_to_hub=True,
    )

    # å‰µå»ºè¨“ç·´å™¨ - åŸºæ–¼ train.py æˆåŠŸæ¨¡å¼
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics_fn,
        tokenizer=processor.feature_extractor,
    )

    # é¡¯ç¤ºè¨“ç·´è³‡è¨Š
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    effective_batch = (
        training_args.per_device_train_batch_size
        * training_args.gradient_accumulation_steps
    )

    print(f"\nğŸ“ˆ æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬è³‡è¨Š:")
    print(f"   æ¨¡å‹åƒæ•¸: {total_params:,}")
    print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
    print(f"   æœ€å¤§æ­¥æ•¸: {QUICK_MAX_STEPS}")
    print(f"   å­¸ç¿’ç‡: {training_args.learning_rate}")
    print(f"   è©•ä¼°é »ç‡: æ¯ {training_args.eval_steps} æ­¥")
    print(f"   ä½¿ç”¨ train.py æˆåŠŸæ¨¡å¼")

    print("\nğŸš€ é–‹å§‹æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬è¨“ç·´...")
    try:
        trainer.train()
        print("âœ… æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬è¨“ç·´å®Œæˆ")

        # ä¿å­˜æ¨¡å‹
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)

        # æœ€çµ‚è©•ä¼°
        metrics = trainer.evaluate()
        print(f"ğŸ¯ æœ€çµ‚ WER: {metrics.get('eval_wer', 'N/A'):.2f}%")

        print(f"\nğŸ’¾ æœ€çµ‚ä¿®æ­£ç‰ˆæœ¬æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")

    except Exception as e:
        print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()

    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()


if __name__ == "__main__":
    main()
